--- git status ---
On branch master
Your branch is up to date with 'origin/master'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1/IMPLEMENTATION_PLAN.md
	modified:   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/commands.py
	modified:   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py
	modified:   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/observations.py
	modified:   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/rewards.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	logs/rsl_rl/booster_t1_rough/2025-10-24_18-41-15/
	source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1_jumping/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1/IMPLEMENTATION_PLAN.md b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1/IMPLEMENTATION_PLAN.md
index ece570e..b1314ab 100644
--- a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1/IMPLEMENTATION_PLAN.md
+++ b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1/IMPLEMENTATION_PLAN.md
@@ -320,16 +320,16 @@ decimation = 4
 ## Implementation Order
 
 1. ✓ Create this documentation file
-2. Create `JumpTargetCommand` class with visualization
-3. Add observation functions (`target_position_rel`, etc.)
-4. Implement jump-specific reward functions
-5. Create `jump_target_curriculum` function
-6. Build `jump_env_cfg.py` environment config
-7. Test with single fixed jump distance (disable curriculum)
-8. Verify observations and rewards are working correctly
-9. Enable curriculum and train stage 1
-10. Progressively train through all curriculum stages
-11. Evaluate performance and iterate on reward weights
+2. ✓ Create `JumpTargetCommand` class with visualization
+3. ✓ Add observation functions (`target_position_rel`, etc.)
+4. ✓ Implement jump-specific reward functions
+5. ✓ Create `jump_target_curriculum` function
+6. ✓ Build `jump_env_cfg.py` environment config
+7. TODO: Test with single fixed jump distance (disable curriculum)
+8. TODO: Verify observations and rewards are working correctly
+9. TODO: Enable curriculum and train stage 1
+10. TODO: Progressively train through all curriculum stages
+11. TODO: Evaluate performance and iterate on reward weights
 
 ---
 
diff --git a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/commands.py b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/commands.py
index fa77c6b..0aab984 100644
--- a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/commands.py
+++ b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/commands.py
@@ -4,10 +4,15 @@
 from __future__ import annotations
 
 import torch
+from dataclasses import MISSING
 from typing import TYPE_CHECKING, Sequence
 
+from isaaclab.assets import Articulation
 from isaaclab.managers import CommandTerm, CommandTermCfg
+from isaaclab.markers import VisualizationMarkers, VisualizationMarkersCfg
+from isaaclab.markers.config import CUBOID_MARKER_CFG
 from isaaclab.utils import configclass
+from isaaclab.utils.math import quat_from_euler_xyz
 
 import robot_lab.tasks.manager_based.locomotion.velocity.mdp as mdp
 
@@ -125,3 +130,168 @@ class DiscreteCommandControllerCfg(CommandTermCfg):
     List of available discrete commands, where each element is an integer.
     Example: [10, 20, 30, 40, 50]
     """
+
+
+class JumpTargetCommand(CommandTerm):
+    """Command generator that generates jump target positions with curriculum learning support.
+
+    This command generator samples 3D target positions (x, y, z) within configurable ranges
+    for training a humanoid robot to jump to target landing zones. It supports:
+    - Horizontal distance curriculum (progressive jump distance)
+    - Height variation (elevated platforms and lowered areas)
+    - Visual target markers for debugging
+    - Success-based resampling
+    """
+
+    cfg: "JumpTargetCommandCfg"
+    """Configuration for the command generator."""
+
+    def __init__(self, cfg: "JumpTargetCommandCfg", env: ManagerBasedEnv):
+        """Initialize the jump target command generator.
+
+        Args:
+            cfg: The configuration of the command generator.
+            env: The environment object.
+        """
+        # Initialize the base class
+        super().__init__(cfg, env)
+
+        # Obtain the robot asset
+        self.robot: Articulation = env.scene[cfg.asset_name]
+
+        # Create buffers to store the command
+        # -- target position in world frame: (x, y, z)
+        self.target_pos_w = torch.zeros(self.num_envs, 3, device=self.device)
+        # -- target position in robot base frame (updated each step)
+        self.target_pos_b = torch.zeros_like(self.target_pos_w)
+        # -- distance from robot to target (horizontal)
+        self.target_distance = torch.zeros(self.num_envs, device=self.device)
+
+        # Initialize curriculum ranges (can be updated by curriculum function)
+        self.horizontal_range = torch.tensor(cfg.horizontal_range, device=self.device)
+        self.height_range = torch.tensor(cfg.height_range, device=self.device)
+
+        # Metrics
+        self.metrics["success_rate"] = torch.zeros(self.num_envs, device=self.device)
+        self.metrics["landing_distance"] = torch.zeros(self.num_envs, device=self.device)
+
+    def __str__(self) -> str:
+        """Return a string representation of the command generator."""
+        msg = "JumpTargetCommand:\n"
+        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
+        msg += f"\tResampling time range: {self.cfg.resampling_time_range}\n"
+        msg += f"\tHorizontal range: {self.horizontal_range.cpu().tolist()}\n"
+        msg += f"\tHeight range: {self.height_range.cpu().tolist()}\n"
+        msg += f"\tSuccess radius: {self.cfg.success_radius}"
+        return msg
+
+    @property
+    def command(self) -> torch.Tensor:
+        """The desired jump target position in base frame. Shape is (num_envs, 3)."""
+        return self.target_pos_b
+
+    def _update_metrics(self):
+        """Update metrics for landing accuracy."""
+        # Calculate horizontal distance to target
+        distance_to_target = torch.norm(
+            self.robot.data.root_pos_w[:, :2] - self.target_pos_w[:, :2],
+            dim=1
+        )
+        self.metrics["landing_distance"] = distance_to_target
+
+        # Check if within success radius
+        success = distance_to_target < self.cfg.success_radius
+        self.metrics["success_rate"] = success.float()
+
+    def _resample_command(self, env_ids: Sequence[int]):
+        """Resample jump target positions for the given environments.
+
+        Args:
+            env_ids: Environment indices for which to resample commands.
+        """
+        # Get current robot positions for the resampling environments
+        robot_pos_w = self.robot.data.root_pos_w[env_ids].clone()
+
+        # Sample random angles for target direction (full 360 degrees)
+        r = torch.empty(len(env_ids), device=self.device)
+        theta = r.uniform_(-torch.pi, torch.pi)
+
+        # Sample horizontal distance from curriculum range
+        horizontal_dist = r.uniform_(self.horizontal_range[0], self.horizontal_range[1])
+
+        # Sample height from curriculum range
+        target_height = r.uniform_(self.height_range[0], self.height_range[1])
+
+        # Calculate target position in world frame
+        self.target_pos_w[env_ids, 0] = robot_pos_w[:, 0] + horizontal_dist * torch.cos(theta)
+        self.target_pos_w[env_ids, 1] = robot_pos_w[:, 1] + horizontal_dist * torch.sin(theta)
+        self.target_pos_w[env_ids, 2] = robot_pos_w[:, 2] + target_height
+
+        # Store horizontal distance for metrics
+        self.target_distance[env_ids] = horizontal_dist
+
+    def _update_command(self):
+        """Update target position in robot base frame."""
+        # Transform target position from world to robot base frame
+        from isaaclab.utils.math import quat_apply_inverse, yaw_quat
+
+        target_vec_w = self.target_pos_w - self.robot.data.root_pos_w[:, :3]
+        self.target_pos_b[:] = quat_apply_inverse(
+            yaw_quat(self.robot.data.root_quat_w),
+            target_vec_w
+        )
+
+    def _set_debug_vis_impl(self, debug_vis: bool):
+        """Set visualization markers for jump targets."""
+        if debug_vis:
+            # Create markers if necessary for the first time
+            if not hasattr(self, "target_visualizer"):
+                self.target_visualizer = VisualizationMarkers(self.cfg.target_visualizer_cfg)
+            # Set visibility to true
+            self.target_visualizer.set_visibility(True)
+        else:
+            if hasattr(self, "target_visualizer"):
+                self.target_visualizer.set_visibility(False)
+
+    def _debug_vis_callback(self, event):
+        """Update visualization markers with current target positions."""
+        # Check if robot is initialized
+        if not self.robot.is_initialized:
+            return
+
+        # Update target marker positions
+        # Create a flat orientation (no rotation)
+        zero_rot = torch.zeros(self.num_envs, device=self.device)
+        target_quat = quat_from_euler_xyz(zero_rot, zero_rot, zero_rot)
+
+        # Visualize the target
+        self.target_visualizer.visualize(
+            translations=self.target_pos_w,
+            orientations=target_quat
+        )
+
+
+@configclass
+class JumpTargetCommandCfg(CommandTermCfg):
+    """Configuration for the jump target command generator."""
+
+    class_type: type = JumpTargetCommand
+
+    asset_name: str = MISSING
+    """Name of the asset in the environment for which the commands are generated."""
+
+    horizontal_range: tuple[float, float] = (0.3, 0.5)
+    """Range for horizontal jump distance in meters. Default: (0.3, 0.5) for curriculum stage 1."""
+
+    height_range: tuple[float, float] = (-0.05, 0.05)
+    """Range for target height relative to starting position in meters.
+    Negative values = lowered areas, Positive values = elevated platforms.
+    Default: (-0.05, 0.05) for curriculum stage 1."""
+
+    success_radius: float = 0.3
+    """Radius tolerance for successful landing in meters. Default: 0.3m."""
+
+    target_visualizer_cfg: VisualizationMarkersCfg = CUBOID_MARKER_CFG.replace(
+        prim_path="/Visuals/Command/jump_target"
+    )
+    """Configuration for the target visualization marker. Shows landing zone as a flat box."""
diff --git a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py
index 93c179e..bfdd2e0 100644
--- a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py
+++ b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py
@@ -58,3 +58,103 @@ def command_levels_vel(
             base_velocity_ranges.lin_vel_y = new_vel_y.tolist()
 
     return torch.tensor(base_velocity_ranges.lin_vel_x[1], device=env.device)
+
+
+def jump_target_curriculum(
+    env: ManagerBasedRLEnv,
+    env_ids: Sequence[int],
+    reward_term_name: str = "reach_target_zone",
+    command_term_name: str = "jump_target",
+    success_threshold: float = 0.7,
+) -> torch.Tensor:
+    """Curriculum learning for jump target commands with progressive difficulty.
+
+    This curriculum progressively increases jump distance and height variation based on
+    the robot's success rate in landing within the target zone.
+
+    Curriculum Stages:
+        - Stage 1: 0.3-0.5m horizontal, ±0.05m height (short level jumps)
+        - Stage 2: 0.5-0.8m horizontal, ±0.10m height (medium jumps, slight elevation)
+        - Stage 3: 0.8-1.2m horizontal, ±0.20m height (long jumps, moderate elevation)
+        - Stage 4: 1.2-1.5m horizontal, ±0.30m height (advanced jumps, large elevation)
+
+    Progression occurs when success rate > 70% over the last 50 episodes.
+
+    Args:
+        env: The learning environment.
+        env_ids: Environment indices for which to update the curriculum.
+        reward_term_name: Name of the reward term tracking jump success. Default: "reach_target_zone".
+        command_term_name: Name of the jump command term. Default: "jump_target".
+        success_threshold: Success rate threshold for progression. Default: 0.7 (70%).
+
+    Returns:
+        Current curriculum level tensor.
+    """
+    # Define curriculum stages
+    CURRICULUM_STAGES = [
+        {"horizontal_range": (0.3, 0.5), "height_range": (-0.05, 0.05)},   # Stage 1
+        {"horizontal_range": (0.5, 0.8), "height_range": (-0.10, 0.10)},   # Stage 2
+        {"horizontal_range": (0.8, 1.2), "height_range": (-0.20, 0.20)},   # Stage 3
+        {"horizontal_range": (1.2, 1.5), "height_range": (-0.30, 0.30)},   # Stage 4
+    ]
+
+    # Initialize curriculum state on first call
+    if not hasattr(env, '_jump_curriculum_level'):
+        env._jump_curriculum_level = 0
+        env._jump_success_history = []
+        env._jump_episode_count = 0
+
+    # Only update curriculum at episode boundaries
+    if len(env_ids) > 0 and env.common_step_counter % env.max_episode_length == 0:
+        # Get success metrics from the reward manager
+        episode_sums = env.reward_manager._episode_sums[reward_term_name]
+
+        # Calculate average success for this episode batch
+        # Success is measured by the reach_target_zone reward
+        avg_reward = torch.mean(episode_sums[env_ids]).item()
+
+        # Track success (normalize by episode length to get average per step)
+        success = avg_reward / env.max_episode_length_s
+
+        # Add to history
+        env._jump_success_history.append(success)
+        env._jump_episode_count += 1
+
+        # Check for progression every 50 episodes
+        if len(env._jump_success_history) >= 50:
+            # Calculate success rate over last 50 episodes
+            recent_success_rate = sum(env._jump_success_history[-50:]) / 50.0
+
+            # Check if we should progress to next stage
+            if (recent_success_rate > success_threshold and
+                env._jump_curriculum_level < len(CURRICULUM_STAGES) - 1):
+
+                # Progress to next stage
+                env._jump_curriculum_level += 1
+
+                print(f"\n{'='*60}")
+                print(f"JUMP CURRICULUM PROGRESSION!")
+                print(f"Success rate: {recent_success_rate:.2%} (threshold: {success_threshold:.2%})")
+                print(f"Advancing to Stage {env._jump_curriculum_level + 1}")
+                print(f"New ranges:")
+                new_stage = CURRICULUM_STAGES[env._jump_curriculum_level]
+                print(f"  Horizontal: {new_stage['horizontal_range']}")
+                print(f"  Height: {new_stage['height_range']}")
+                print(f"{'='*60}\n")
+
+                # Update command generator ranges
+                jump_command = env.command_manager._terms[command_term_name]
+                jump_command.horizontal_range = torch.tensor(
+                    new_stage['horizontal_range'],
+                    device=env.device
+                )
+                jump_command.height_range = torch.tensor(
+                    new_stage['height_range'],
+                    device=env.device
+                )
+
+                # Reset success history for new stage
+                env._jump_success_history = []
+
+    # Return current curriculum level as tensor
+    return torch.tensor(env._jump_curriculum_level, device=env.device)
diff --git a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/observations.py b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/observations.py
index 8a862a9..fd97c89 100644
--- a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/observations.py
+++ b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/observations.py
@@ -8,6 +8,7 @@ from typing import TYPE_CHECKING
 
 from isaaclab.assets import Articulation
 from isaaclab.managers import SceneEntityCfg
+from isaaclab.sensors import ContactSensor
 
 if TYPE_CHECKING:
     from isaaclab.envs import ManagerBasedEnv, ManagerBasedRLEnv
@@ -32,3 +33,131 @@ def phase(env: ManagerBasedRLEnv, cycle_time: float) -> torch.Tensor:
     phase = env.episode_length_buf[:, None] * env.step_dt / cycle_time
     phase_tensor = torch.cat([torch.sin(2 * torch.pi * phase), torch.cos(2 * torch.pi * phase)], dim=-1)
     return phase_tensor
+
+
+# ==============================================================================
+# Jump-Specific Observations
+# ==============================================================================
+
+
+def target_position_rel(env: ManagerBasedRLEnv, command_name: str = "jump_target") -> torch.Tensor:
+    """Returns 3D vector from robot base to target in robot body frame.
+
+    This observation helps the robot understand where to jump in its local coordinate system.
+
+    Args:
+        env: The learning environment.
+        command_name: Name of the jump target command. Default: "jump_target".
+
+    Returns:
+        Relative target position in body frame. Shape: (num_envs, 3).
+    """
+    # Get jump target command (already in body frame)
+    target_pos_b = env.command_manager.get_command(command_name)
+    return target_pos_b
+
+
+def target_distance_height(env: ManagerBasedRLEnv, command_name: str = "jump_target") -> torch.Tensor:
+    """Returns horizontal distance and height difference to target.
+
+    This observation provides a compact representation of the jump challenge:
+    - Horizontal distance: how far to jump (sqrt(dx^2 + dy^2))
+    - Height difference: how high/low to jump (target_z - base_z)
+
+    Args:
+        env: The learning environment.
+        command_name: Name of the jump target command. Default: "jump_target".
+
+    Returns:
+        Tensor containing [horizontal_distance, height_difference]. Shape: (num_envs, 2).
+    """
+    # Get jump target command in body frame
+    target_pos_b = env.command_manager.get_command(command_name)
+
+    # Calculate horizontal distance (XY plane)
+    horizontal_distance = torch.norm(target_pos_b[:, :2], dim=1, keepdim=True)
+
+    # Height difference is just the Z component
+    height_difference = target_pos_b[:, 2:3]
+
+    return torch.cat([horizontal_distance, height_difference], dim=1)
+
+
+def is_airborne(env: ManagerBasedRLEnv, sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces")) -> torch.Tensor:
+    """Returns binary indicator of whether robot is airborne (all feet off ground).
+
+    This observation helps the robot distinguish between ground contact and flight phases.
+
+    Args:
+        env: The learning environment.
+        sensor_cfg: Configuration for the contact sensor. Default: SceneEntityCfg("contact_forces").
+
+    Returns:
+        Binary indicator: 1.0 if airborne, 0.0 if on ground. Shape: (num_envs, 1).
+    """
+    # Get contact sensor
+    contact_sensor: ContactSensor = env.scene[sensor_cfg.name]
+
+    # Get contact forces for feet (sensor configured to monitor feet)
+    # Shape: (num_envs, num_feet)
+    contact_forces = contact_sensor.data.net_forces_w_history[:, :, sensor_cfg.body_ids, 2].max(dim=1)[0]
+
+    # Check if all feet have zero contact (airborne = all contacts below threshold)
+    # Use small threshold to account for numerical noise
+    contact_threshold = 1.0  # Newtons
+    feet_in_contact = contact_forces > contact_threshold
+
+    # Airborne if no feet are in contact
+    airborne = (~feet_in_contact.any(dim=1)).float().unsqueeze(1)
+
+    return airborne
+
+
+def feet_position_to_target(
+    env: ManagerBasedRLEnv,
+    command_name: str = "jump_target",
+    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
+) -> torch.Tensor:
+    """Returns relative position of both feet to target center.
+
+    This observation helps the robot understand foot placement during landing approach.
+
+    Args:
+        env: The learning environment.
+        command_name: Name of the jump target command. Default: "jump_target".
+        asset_cfg: Configuration for the robot asset. Default: SceneEntityCfg("robot").
+
+    Returns:
+        Relative foot positions to target: [left_foot_xyz, right_foot_xyz]. Shape: (num_envs, 6).
+    """
+    # Get robot asset
+    asset: Articulation = env.scene[asset_cfg.name]
+
+    # Get target position in world frame from command manager
+    # Need to access the command manager's internal target position
+    jump_command = env.command_manager._terms[command_name]
+    target_pos_w = jump_command.target_pos_w
+
+    # Get feet body indices (assumes body names contain "foot")
+    # This should match the foot_link_name pattern in the environment config
+    foot_body_names = asset_cfg.body_names
+    if foot_body_names is None:
+        # Fallback: try to find feet automatically
+        foot_body_names = [".*foot.*"]
+
+    foot_body_ids = asset_cfg.body_ids
+
+    # Get feet positions in world frame
+    # Shape: (num_envs, num_feet, 3)
+    feet_pos_w = asset.data.body_pos_w[:, foot_body_ids, :]
+
+    # Calculate relative position of each foot to target
+    # Assuming 2 feet (left and right) for a biped
+    # feet_pos_w shape: (num_envs, 2, 3)
+    # target_pos_w shape: (num_envs, 3)
+    feet_to_target = feet_pos_w - target_pos_w.unsqueeze(1)
+
+    # Flatten to (num_envs, 6) - [left_x, left_y, left_z, right_x, right_y, right_z]
+    feet_to_target_flat = feet_to_target.reshape(env.num_envs, -1)
+
+    return feet_to_target_flat
diff --git a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/rewards.py b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/rewards.py
index 4d00e7b..350a0f6 100644
--- a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/rewards.py
+++ b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/rewards.py
@@ -680,3 +680,259 @@ def flat_orientation_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = Scen
     reward = torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)
     reward *= torch.clamp(-env.scene["robot"].data.projected_gravity_b[:, 2], 0, 0.7) / 0.7
     return reward
+
+
+# ==============================================================================
+# Jump-Specific Rewards
+# ==============================================================================
+
+
+def reach_target_zone(
+    env: ManagerBasedRLEnv,
+    command_name: str = "jump_target",
+    tolerance: float = 0.3,
+    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
+) -> torch.Tensor:
+    """Large positive reward for landing within target zone.
+
+    Uses exponential kernel to reward landing close to the target.
+    Only triggered when robot lands (feet make contact after flight).
+
+    Args:
+        env: The learning environment.
+        command_name: Name of the jump target command. Default: "jump_target".
+        tolerance: Success radius in meters. Default: 0.3m.
+        sensor_cfg: Configuration for the contact sensor. Default: SceneEntityCfg("contact_forces").
+
+    Returns:
+        Reward tensor. Shape: (num_envs,).
+    """
+    # Get robot asset
+    asset: RigidObject = env.scene["robot"]
+
+    # Get target position from command manager
+    jump_command = env.command_manager._terms[command_name]
+    target_pos_w = jump_command.target_pos_w
+
+    # Calculate horizontal distance to target
+    distance_to_target = torch.norm(
+        asset.data.root_pos_w[:, :2] - target_pos_w[:, :2],
+        dim=1
+    )
+
+    # Exponential kernel reward based on distance
+    reward = torch.exp(-distance_to_target**2 / tolerance**2)
+
+    # Only reward when feet are in contact (landing phase)
+    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
+    feet_in_contact = contact_sensor.data.net_forces_w_history[:, :, sensor_cfg.body_ids, 2].max(dim=1)[0] > 1.0
+    any_foot_contact = feet_in_contact.any(dim=1).float()
+
+    reward = reward * any_foot_contact
+
+    return reward
+
+
+def target_progress(
+    env: ManagerBasedRLEnv,
+    command_name: str = "jump_target",
+    std: float = 0.5,
+) -> torch.Tensor:
+    """Shaped reward for reducing horizontal distance to target.
+
+    Tracks distance reduction over time to guide robot toward target.
+
+    Args:
+        env: The learning environment.
+        command_name: Name of the jump target command. Default: "jump_target".
+        std: Standard deviation for exponential kernel. Default: 0.5.
+
+    Returns:
+        Reward tensor. Shape: (num_envs,).
+    """
+    # Get robot asset
+    asset: RigidObject = env.scene["robot"]
+
+    # Get target position from command manager
+    jump_command = env.command_manager._terms[command_name]
+    target_pos_w = jump_command.target_pos_w
+
+    # Calculate current horizontal distance to target
+    current_distance = torch.norm(
+        asset.data.root_pos_w[:, :2] - target_pos_w[:, :2],
+        dim=1
+    )
+
+    # Store previous distance (initialize if needed)
+    if not hasattr(env, '_prev_target_distance'):
+        env._prev_target_distance = current_distance.clone()
+
+    # Calculate progress (reduction in distance)
+    progress = env._prev_target_distance - current_distance
+
+    # Update previous distance for next step
+    env._prev_target_distance = current_distance.clone()
+
+    # Shaped reward based on progress
+    reward = progress / std
+
+    return reward
+
+
+def flight_phase_quality(
+    env: ManagerBasedRLEnv,
+    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
+    min_height: float = 0.1,
+    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
+) -> torch.Tensor:
+    """Reward proper takeoff with both feet leaving ground.
+
+    Provides bonus for achieving minimum flight height.
+
+    Args:
+        env: The learning environment.
+        sensor_cfg: Configuration for the contact sensor. Default: SceneEntityCfg("contact_forces").
+        min_height: Minimum flight height for bonus reward in meters. Default: 0.1m.
+        asset_cfg: Configuration for the robot asset. Default: SceneEntityCfg("robot").
+
+    Returns:
+        Reward tensor. Shape: (num_envs,).
+    """
+    # Get contact sensor
+    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
+
+    # Check if all feet are off the ground (airborne)
+    contact_forces = contact_sensor.data.net_forces_w_history[:, :, sensor_cfg.body_ids, 2].max(dim=1)[0]
+    contact_threshold = 1.0
+    feet_in_contact = contact_forces > contact_threshold
+    airborne = ~feet_in_contact.any(dim=1)
+
+    # Get robot height (Z velocity as proxy for flight quality)
+    asset: RigidObject = env.scene[asset_cfg.name]
+    vertical_velocity = asset.data.root_lin_vel_w[:, 2]
+
+    # Reward being airborne
+    reward = airborne.float()
+
+    # Bonus for upward velocity during flight
+    reward += torch.clamp(vertical_velocity, min=0.0, max=min_height * 10.0)
+
+    return reward
+
+
+def landing_stability(
+    env: ManagerBasedRLEnv,
+    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
+    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
+) -> torch.Tensor:
+    """Reward stable landing with low angular velocity and upright orientation.
+
+    Only applies during landing phase (when feet make contact).
+
+    Args:
+        env: The learning environment.
+        sensor_cfg: Configuration for the contact sensor. Default: SceneEntityCfg("contact_forces").
+        asset_cfg: Configuration for the robot asset. Default: SceneEntityCfg("robot").
+
+    Returns:
+        Reward tensor. Shape: (num_envs,).
+    """
+    # Get robot asset
+    asset: RigidObject = env.scene[asset_cfg.name]
+
+    # Get contact sensor
+    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
+
+    # Check if feet are in contact (landing phase)
+    contact_forces = contact_sensor.data.net_forces_w_history[:, :, sensor_cfg.body_ids, 2].max(dim=1)[0]
+    feet_in_contact = contact_forces > 1.0
+    any_foot_contact = feet_in_contact.any(dim=1).float()
+
+    # Reward low angular velocity (stable landing)
+    ang_vel_penalty = torch.sum(torch.square(asset.data.root_ang_vel_b[:, :2]), dim=1)
+    ang_vel_reward = torch.exp(-ang_vel_penalty)
+
+    # Reward upright orientation
+    orientation_reward = 1.0 - torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)
+
+    # Combine rewards, only during landing
+    reward = (ang_vel_reward + orientation_reward) * any_foot_contact
+
+    return reward
+
+
+def smooth_flight_trajectory(
+    env: ManagerBasedRLEnv,
+    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
+    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
+) -> torch.Tensor:
+    """Penalize excessive rotation during flight phase.
+
+    Encourages controlled flight without tumbling.
+
+    Args:
+        env: The learning environment.
+        sensor_cfg: Configuration for the contact sensor. Default: SceneEntityCfg("contact_forces").
+        asset_cfg: Configuration for the robot asset. Default: SceneEntityCfg("robot").
+
+    Returns:
+        Penalty tensor (negative values). Shape: (num_envs,).
+    """
+    # Get robot asset
+    asset: RigidObject = env.scene[asset_cfg.name]
+
+    # Get contact sensor
+    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
+
+    # Check if airborne
+    contact_forces = contact_sensor.data.net_forces_w_history[:, :, sensor_cfg.body_ids, 2].max(dim=1)[0]
+    feet_in_contact = contact_forces > 1.0
+    airborne = (~feet_in_contact.any(dim=1)).float()
+
+    # Penalize angular velocity during flight
+    ang_vel_magnitude = torch.norm(asset.data.root_ang_vel_b, dim=1)
+    penalty = ang_vel_magnitude * airborne
+
+    return penalty
+
+
+def landing_orientation(
+    env: ManagerBasedRLEnv,
+    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
+    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
+    target_orientation: list[float] = [0, 0, 0, 1],
+) -> torch.Tensor:
+    """Reward landing with correct upright orientation.
+
+    Uses quaternion distance to measure orientation error.
+
+    Args:
+        env: The learning environment.
+        sensor_cfg: Configuration for the contact sensor. Default: SceneEntityCfg("contact_forces").
+        asset_cfg: Configuration for the robot asset. Default: SceneEntityCfg("robot").
+        target_orientation: Target quaternion [x, y, z, w]. Default: [0, 0, 0, 1] (upright).
+
+    Returns:
+        Reward tensor. Shape: (num_envs,).
+    """
+    # Get robot asset
+    asset: RigidObject = env.scene[asset_cfg.name]
+
+    # Get contact sensor
+    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
+
+    # Check if feet are in contact (landing phase)
+    contact_forces = contact_sensor.data.net_forces_w_history[:, :, sensor_cfg.body_ids, 2].max(dim=1)[0]
+    feet_in_contact = contact_forces > 1.0
+    any_foot_contact = feet_in_contact.any(dim=1).float()
+
+    # Calculate orientation error (using projected gravity as simpler measure)
+    # Perfect upright = projected_gravity_b = [0, 0, -1]
+    # Error is when x and y components are non-zero
+    orientation_error = torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)
+    orientation_reward = torch.exp(-orientation_error / 0.5**2)
+
+    # Only reward during landing
+    reward = orientation_reward * any_foot_contact
+
+    return reward