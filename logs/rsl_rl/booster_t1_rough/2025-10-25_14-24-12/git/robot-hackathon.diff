--- git status ---
On branch master
Your branch is up to date with 'origin/master'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1_jumping/jump_env_cfg.py
	modified:   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	logs/rsl_rl/booster_t1_rough/2025-10-25_10-24-02/
	logs/rsl_rl/booster_t1_rough/2025-10-25_10-28-48/
	logs/rsl_rl/booster_t1_rough/2025-10-25_10-33-19/model_150.pt
	logs/rsl_rl/booster_t1_rough/2025-10-25_11-32-24/
	logs/rsl_rl/booster_t1_rough/2025-10-25_11-42-27/
	logs/rsl_rl/booster_t1_rough/2025-10-25_12-04-59/
	logs/rsl_rl/booster_t1_rough/2025-10-25_12-09-50/
	logs/rsl_rl/booster_t1_rough/2025-10-25_12-14-29/
	logs/rsl_rl/booster_t1_rough/2025-10-25_12-29-10/
	logs/rsl_rl/booster_t1_rough/2025-10-25_12-38-36/
	logs/rsl_rl/booster_t1_rough/2025-10-25_12-47-48/
	logs/rsl_rl/booster_t1_rough/2025-10-25_13-00-16/
	logs/rsl_rl/booster_t1_rough/2025-10-25_13-08-33/
	logs/rsl_rl/booster_t1_rough/2025-10-25_13-21-46/
	logs/rsl_rl/booster_t1_rough/2025-10-25_13-26-57/
	logs/rsl_rl/booster_t1_rough/2025-10-25_13-30-06/
	logs/rsl_rl/booster_t1_rough/2025-10-25_13-36-00/
	logs/rsl_rl/booster_t1_rough/2025-10-25_13-38-58/
	logs/rsl_rl/booster_t1_rough/2025-10-25_14-17-55/
	logs/rsl_rl/booster_t1_rough/2025-10-25_14-24-12/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1_jumping/jump_env_cfg.py b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1_jumping/jump_env_cfg.py
index fc4fb58..94260be 100644
--- a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1_jumping/jump_env_cfg.py
+++ b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/humanoid/booster_t1_jumping/jump_env_cfg.py
@@ -136,23 +136,52 @@ class BoosterT1JumpEnvCfg(LocomotionVelocityRoughEnvCfg):
         # Rewards Configuration
         # ======================================================================================
 
-        # --- MINIMAL JUMP REWARDS (Optimized for performance) ---
-        # Key: Compute expensive checks ONLY at episode boundaries, not every step
+        # --- OPTIMIZED JUMP REWARDS (Balanced performance ~150K steps/s) ---
+        # Key rewards for landing accuracy, dual-foot mechanics, and stability
 
-        # Simple distance reward (computed from robot position - no sensor queries!)
+        # 1. Landing Accuracy - Large reward for landing near target
+        self.rewards.reach_target_zone = RewTerm(
+            func=mdp.reach_target_zone,
+            weight=50.0,  # Main task reward
+            params={
+                "command_name": "jump_target",
+                "tolerance": 0.5,
+                "sensor_cfg": SceneEntityCfg("contact_forces", body_names=[self.foot_link_name]),
+            },
+        )
+
+        # 2. Approach Target - Continuous reward for reducing distance
         self.rewards.approach_target = RewTerm(
-            func=mdp.target_progress,  # Uses simple position difference
-            weight=1.0,
+            func=mdp.target_progress,
+            weight=2.0,  # Increased from 1.0 for stronger guidance
             params={"command_name": "jump_target", "std": 1.0},
         )
 
-        # Disable ALL contact sensor-based rewards (these query sensors every step - VERY expensive!)
+        # 3. Dual-Foot Landing - Encourage landing with both feet
+        self.rewards.dual_foot_landing = RewTerm(
+            func=mdp.dual_foot_landing,
+            weight=10.0,
+            params={
+                "sensor_cfg": SceneEntityCfg("contact_forces", body_names=[self.foot_link_name]),
+                "time_window": 0.2,
+            },
+        )
+
+        # 4. Landing Stability - Reward upright landing without tumbling
+        self.rewards.landing_stability = RewTerm(
+            func=mdp.landing_stability,
+            weight=8.0,
+            params={
+                "sensor_cfg": SceneEntityCfg("contact_forces", body_names=[self.foot_link_name]),
+                "asset_cfg": SceneEntityCfg("robot"),
+            },
+        )
+
+        # Disable less critical rewards (keep overhead low)
         self.rewards.flight_phase_quality = None
-        self.rewards.landing_stability = None
         self.rewards.smooth_flight_trajectory = None
-        self.rewards.landing_orientation = None
+        self.rewards.landing_orientation = None  # Covered by landing_stability
         self.rewards.dual_foot_takeoff = None
-        self.rewards.dual_foot_landing = None
 
         # --- MODIFIED REWARDS (Reduced penalties for jumping) ---
 
diff --git a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py
index 7d4b9b8..a3f5789 100644
--- a/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py
+++ b/source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/mdp/curriculums.py
@@ -152,7 +152,6 @@ def jump_target_curriculum(
         env._jump_curriculum_level = 0  # Current highest unlocked stage
         env._jump_current_stage_assignments = torch.zeros(env.num_envs, dtype=torch.int32, device=env.device)
         env._jump_evaluation_cycle = 0
-        env._jump_iteration_counter = 0
 
         # Simplified tracking: accumulate successes per stage (no rolling buffer)
         env._jump_stage_successes = torch.zeros(len(CURRICULUM_STAGES), device=env.device)
@@ -172,8 +171,11 @@ def jump_target_curriculum(
         env._jump_success_threshold = success_threshold
         env._jump_regression_threshold = regression_threshold
 
-    # Update success tracking at episode boundaries (FULLY VECTORIZED - minimal overhead)
-    if len(env_ids) > 0 and env.common_step_counter % env.max_episode_length == 0:
+        # Logging: Track episodes
+        env._jump_total_episodes = 0
+
+    # Update success tracking whenever environments reset
+    if len(env_ids) > 0:
         # Get success metrics from the reward manager
         episode_sums = env.reward_manager._episode_sums[reward_term_name]
 
@@ -188,10 +190,39 @@ def jump_target_curriculum(
         env._jump_stage_successes.scatter_add_(0, stage_indices.long(), successes)
         env._jump_stage_total.scatter_add_(0, stage_indices.long(), torch.ones_like(successes))
 
-        env._jump_iteration_counter += 1
+        # Track total episodes for logging
+        env._jump_total_episodes += len(env_ids)
+
+    # Increment call counter for logging
+    if not hasattr(env, '_jump_call_counter'):
+        env._jump_call_counter = 0
+    env._jump_call_counter += 1
 
-    # Evaluate curriculum every 50 iterations (REDUCED FREQUENCY for less Python overhead)
-    if env._jump_iteration_counter > 0 and env._jump_iteration_counter % 50 == 0:
+    # Progress logging every 100 curriculum calls (roughly every few training iterations)
+    # With 16K envs, curriculum is called very frequently, so we need a higher threshold
+    if env._jump_call_counter % 100 == 0:
+        num_unlocked_stages = env._jump_curriculum_level + 1
+
+        # Calculate current success rate across all unlocked stages
+        if torch.min(env._jump_stage_total[:num_unlocked_stages]) >= 1:
+            stage_success_rates = env._jump_stage_successes[:num_unlocked_stages] / (env._jump_stage_total[:num_unlocked_stages] + 1e-8)
+            overall_success = torch.mean(stage_success_rates).item()
+
+            # Total rollouts collected across all unlocked stages
+            total_rollouts = int(torch.sum(env._jump_stage_total[:num_unlocked_stages]).item())
+            min_rollouts_needed = 10 * num_unlocked_stages  # Need 10 rollouts per stage for evaluation
+
+            # Get current and next stage names
+            current_stage = env._jump_curriculum_level
+            next_stage = min(current_stage + 1, len(CURRICULUM_STAGES) - 1)
+
+            print(f"\n[Curriculum] Stage {current_stage} ‚Üí {next_stage} | "
+                  f"Success: {overall_success:.1%} (need {success_threshold:.0%}) | "
+                  f"Rollouts: {total_rollouts}/{min_rollouts_needed} | "
+                  f"Episodes: {env._jump_total_episodes}")
+
+    # Evaluate curriculum every 500 calls (balanced frequency for 16K envs)
+    if env._jump_call_counter % 500 == 0:
         num_unlocked_stages = env._jump_curriculum_level + 1
 
         # Check if we have enough data (at least 10 rollouts per unlocked stage)
@@ -203,20 +234,47 @@ def jump_target_curriculum(
             # Overall success rate is average across all unlocked stages
             overall_success_rate = torch.mean(stage_success_rates).item()  # ONE sync point
 
+            # Print detailed statistics before making decision
+            print(f"\n{'='*70}")
+            print(f"[Curriculum Evaluation - Episodes: {env._jump_total_episodes}]")
+            print(f"{'='*70}")
+
+            # Per-stage breakdown
+            for i in range(num_unlocked_stages):
+                total = int(env._jump_stage_total[i].item())
+                success = int(env._jump_stage_successes[i].item())
+                success_rate = stage_success_rates[i].item()
+
+                h_range = CURRICULUM_STAGES[i]['horizontal_range']
+                z_range = CURRICULUM_STAGES[i]['height_range']
+
+                status = "‚úì UNLOCKED" if i <= env._jump_curriculum_level else ""
+                current_marker = " ‚Üê CURRENT" if i == env._jump_curriculum_level else ""
+
+                print(f"  Stage {i}: {h_range[0]:.1f}-{h_range[1]:.1f}m, ¬±{z_range[1]:.2f}m | "
+                      f"{total} episodes | {success_rate:.1%} success {status}{current_marker}")
+
+            print(f"\nOverall Success Rate: {overall_success_rate:.1%} (threshold: {success_threshold:.0%})")
+            print(f"Total Episodes: {env._jump_total_episodes}")
+
             # Progression/regression logic (minimal Python-side logic)
             if overall_success_rate > success_threshold and env._jump_curriculum_level < len(CURRICULUM_STAGES) - 1:
                 env._jump_curriculum_level += 1
                 # Reset counters when advancing
                 env._jump_stage_successes.zero_()
                 env._jump_stage_total.zero_()
-                print(f"[Curriculum] ‚Üí Stage {env._jump_curriculum_level} | Success: {overall_success_rate:.1%}")
+                print(f"\nüéØ ADVANCING TO STAGE {env._jump_curriculum_level}!")
 
             elif overall_success_rate <= regression_threshold and env._jump_curriculum_level > 0:
                 env._jump_curriculum_level -= 1
                 # Reset counters when regressing
                 env._jump_stage_successes.zero_()
                 env._jump_stage_total.zero_()
-                print(f"[Curriculum] ‚Üê Stage {env._jump_curriculum_level} | Success: {overall_success_rate:.1%}")
+                print(f"\n‚ö†Ô∏è  REGRESSING TO STAGE {env._jump_curriculum_level}")
+            else:
+                print(f"\n‚Üí Staying at Stage {env._jump_curriculum_level}")
+
+            print(f"{'='*70}\n")
 
             env._jump_evaluation_cycle += 1
 